{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from class_vis import prettyPicture\n",
    "\n",
    "\n",
    "#import copy\n",
    "\n",
    "#import pylab as pl\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "#from sklearn.preprocessing import *\n",
    "#from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38973, 60)\n",
      "(6878, 60)\n",
      "0.9011340505961035\n"
     ]
    }
   ],
   "source": [
    "def normalize(inputData):\n",
    "        #return (inputData - inputData.mean()) / inputData.std()\n",
    "        return (inputData - inputData.min()) / (inputData.max() - inputData.min())\n",
    "def getdata():\n",
    "    #########train############################\n",
    "    train=pd.read_csv('/home/ayush/subj2/post_ml/assign_2/train.csv')\n",
    "    train['bias'] = pd.DataFrame(data=np.ones(len(train.index)))\n",
    "    train_y=train['salary']\n",
    "    train_x=train.drop('salary',1)\n",
    "    train_x=train_x.drop('race',1)\n",
    "    train_x=train_x.drop('native-country',1)\n",
    "    #########test############################\n",
    "    test=pd.read_csv('/home/ayush/subj2/post_ml/assign_2/test.csv')\n",
    "    test['bias'] = pd.DataFrame(data=np.ones(len(test.index)))\n",
    "    ids=test['id'].values\n",
    "    test=test.drop('race',1)\n",
    "    test=test.drop('native-country',1)\n",
    "    #################normalizattion##############3\n",
    "    numericalColumns = ('age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week')\n",
    "    for i in numericalColumns:\n",
    "        train_x[i] = normalize(train_x[i])\n",
    "        test[i] = normalize(test[i])\n",
    "    #################Dummies##################\n",
    "    #########train############################\n",
    "    train_x=pd.get_dummies(train_x)\n",
    "    train_x=train_x.drop('workclass_ ?',1)\n",
    "    train_x=train_x.drop('id',1)\n",
    "    train_x=train_x.drop('occupation_ ?',1)\n",
    "    #########test############################\n",
    "    test=pd.get_dummies(test)\n",
    "    test=test.drop('workclass_ ?',1)\n",
    "    test=test.drop('occupation_ ?',1)\n",
    "    test=test.drop('id',1)\n",
    "    #############################\n",
    "    diff=set(list(train_x)).symmetric_difference(set(list(test)))\n",
    "    #print(list(diff))\n",
    "    for i in diff:\n",
    "        if i in train_x:\n",
    "            train_x=train_x.drop(i,1)\n",
    "        else:\n",
    "            test=test.drop(i,1)\n",
    "    #print(test['id'])\n",
    "    #data.to_csv(name, sep=',', encoding='utf-8')\n",
    "    return train_x,train_y,test,ids\n",
    "    #print(train_x)\n",
    "    \n",
    "\n",
    "train_x,train_y,test,ids=getdata()\n",
    "#################limiting the data set##########\n",
    "#print(list(train_x))\n",
    "phi=train_x.values\n",
    "phi=phi[0:,:]\n",
    "y=train_y.values\n",
    "y=y[0:]\n",
    "#print(y)\n",
    "test_=test.values\n",
    "test_=test_[0:,:]\n",
    "print(phi.shape)\n",
    "print(test_.shape)\n",
    "\n",
    "\n",
    "\n",
    "###############accuracy###########################\n",
    "def accuracy(result,expected):\n",
    "    count=0\n",
    "    for i in range(0,len(result)):\n",
    "        if result[i]==expected[i]:\n",
    "            count+=1\n",
    "    return (count/len(expected))\n",
    "################SVM################\n",
    "'''\n",
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(phi,y)\n",
    "print(\"here\")\n",
    "svm_pred=clf.predict(test_)\n",
    "print(\"here\")\n",
    "print(svm_pred)'''\n",
    "###############Naive Bayes#########\n",
    "gnb = GaussianNB()\n",
    "gnb_pred = gnb.fit(phi,y).predict(test_)\n",
    "#print (gnb_pred)\n",
    "############## Bagging meta-estimator#########\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n",
    "bagging_pred=bagging.fit(phi,y).predict(test_)\n",
    "#print(bagging_pred)\n",
    "##############AdaBoost#######################\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "ada_pred=ada.fit(phi,y).predict(test_)\n",
    "#print(ada_pred)\n",
    "acc = accuracy(ada_pred,bagging_pred)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878,)\n",
      "(6878,)\n",
      "0.6032276824658331\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#############Neural Network#################\n",
    "rows,col=phi.shape\n",
    "\n",
    "\n",
    "#####constant declaration######\n",
    "num_of_input_neurons=60\n",
    "num_of_hidden_layer1_neurons=20\n",
    "num_of_hidden_layer2_neurons=20\n",
    "num_of_output_layer_neurons=1\n",
    "\n",
    "\n",
    "\n",
    "#####initilization of weight matrix#####\n",
    "wt1=2*np.random.random((num_of_input_neurons,num_of_hidden_layer1_neurons))-1\n",
    "wt2=2*np.random.random((num_of_hidden_layer1_neurons,num_of_hidden_layer2_neurons))-1\n",
    "wt3=2*np.random.random((num_of_hidden_layer2_neurons,num_of_output_layer_neurons))-1\n",
    "#print(wt1)0\n",
    "#print(wt1.shape)\n",
    "eta=0.0001\n",
    "\n",
    "'''####Sigmoid Function#####\n",
    "def sigmoid(x,derivative):\n",
    "    if(derivative==1):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))'''\n",
    "\n",
    "def sigmoid(x,derivative):\n",
    "    if(derivative==1):\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "#######tanh############\n",
    "\n",
    "'''def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "'''\n",
    "\n",
    "\n",
    "data=phi\n",
    "train_y=np.reshape(y,(rows,1))\n",
    "#print(train_y)\n",
    "train_x= np.array(data)\n",
    "#neural(Input,expected)\n",
    "#print(wt1)\n",
    "for j in range(1,1000):\n",
    "    ###forward_pass###\n",
    "    layer1_output=sigmoid(np.dot(train_x,wt1),0)\n",
    "    layer2_output=sigmoid(np.dot(layer1_output,wt2),0)\n",
    "    layer3_output=sigmoid(np.dot(layer2_output,wt3),0)\n",
    "    #print(layer1_output.shape,layer2_output.shape,layer3_output.shape)\n",
    "    ###back_pass###\n",
    "    layer3_error=layer3_output-train_y#here train_y will have 2 column as 2 neurons assumed in last layer\n",
    "    #print(layer3_output)\n",
    "    #print(train_y)\n",
    "    layer3_senstivity = layer3_error*sigmoid(layer3_output,1)\n",
    "    \n",
    "    #print(layer3_senstivity.shape,wt3.shape)\n",
    "    layer2_error=layer3_senstivity.dot(wt3.T)\n",
    "    #print(layer2_error.shape)\n",
    "    layer2_senstivity=layer2_error*(sigmoid(layer2_output,1))\n",
    "    #print(layer2_senstivity)\n",
    "    layer1_error=layer2_senstivity.dot(wt2.T)\n",
    "    layer1_senstivity=layer1_error*(sigmoid(layer1_output,1))\n",
    "    wt3=wt3-(eta*layer2_output.T.dot(layer3_senstivity))\n",
    "    wt2=wt2-eta*layer1_output.T.dot(layer2_senstivity)\n",
    "    wt1=wt1-eta*(train_x.T).dot(layer1_senstivity)\n",
    "\n",
    "\n",
    "\n",
    "layer1= sigmoid(np.dot(test_,wt1),0)\n",
    "layer2= sigmoid(np.dot(layer1,wt2),0)\n",
    "layer3= sigmoid(np.dot(layer2,wt3),0)\n",
    "nn_pred=[]\n",
    "layer3=layer3.ravel()\n",
    "print(layer3.shape)\n",
    "print(gnb_pred.shape)\n",
    "for i in range(0,len(layer3)):\n",
    "    if layer3[i] > 0:\n",
    "        nn_pred.append(1)\n",
    "    else:\n",
    "        nn_pred.append(0)\n",
    "#print (nn_pred)\n",
    "#print(list(gnb_pred))\n",
    "\n",
    "\n",
    "\n",
    "#acc = accuracy_score(nn_pred,gnb_pred)\n",
    "#print (acc)\n",
    "print(accuracy(nn_pred,gnb_pred))\n",
    "\n",
    "\n",
    "\n",
    "##################saving result in a file#################################\n",
    "result=np.column_stack((ids,gnb_pred))\n",
    "np.savetxt(\"prediction.csv\",result, delimiter=',',fmt=\"%d,%d\",header=\"id,salary\",comments ='')#saving result in a file\n",
    "\n",
    "    #eturn wt1,wt2,wt3\n",
    "    \n",
    "#print(sigmoid(np.dot([[1,2,3],[4,5,6],[7,8,9]],wt1),0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(percent_train):\n",
    "    days=10\n",
    "    #data = np.genfromtxt('/home/ayush/subj2/post_ml/assign_2/train.csv', delimiter=',')\n",
    "    #print(data[1]) print nan values for strings\n",
    "    \n",
    "    train_data =pd.read_csv('/home/ayush/subj2/post_ml/assign_2/train.csv')\n",
    "    #print(train_data)\n",
    "    test_data = pd.read_csv('/home/ayush/subj2/post_ml/assign_2/test.csv')\n",
    "    train_data=train_data.dropna()\n",
    "    cols_to_drop = [ 'id', 'age', 'fnlwgt', 'education-num','capital-gain','capital-loss','hours-per-week','salary']\n",
    "    cat_dict = train_data.drop( cols_to_drop, axis = 1 ).to_dict( orient = 'records' )\n",
    "    #print(cat_dict)\n",
    "    vectorizer = DV( sparse = False )\n",
    "    vec_x_cat_train = vectorizer.fit_transform( cat_dict )\n",
    "    print(vec_x_cat_train)\n",
    "    np.savetxt(\"train_\",vec_x_cat_train, delimiter=',',fmt=\"%d,%.2f\",comments ='')\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \n",
    "    #age=train_data['age']\n",
    "    #a=plt.plot(label='age')\n",
    "    #plt.scatter(train_data.age,train_data.workclass)\n",
    "    #plt.show()\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC(kernel=\"linear\")\n",
    "    clf.fit(features_train,labels_train)\n",
    "    pred=clf.predict(features_test)\n",
    "get_data(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def result(weight,train_x,test_x,ids,file_name):\n",
    "    predicted=numpy.dot(test_x,weight).astype(float)#predicted value\n",
    "    result=numpy.column_stack((ids,predicted))\n",
    "    numpy.savetxt(file_name,result, delimiter=',',fmt=\"%d,%.2f\",header=\"ID,MEDV\",comments ='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
